{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate test variables\n",
    "prob_control = 0.10\n",
    "prob_treatment = 0.12\n",
    "\n",
    "size_control = 1000\n",
    "size_treatment = 1000\n",
    "\n",
    "alpha = 0.05 # Allowance for type 1 error. P value significance threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypotheses\n",
    "\n",
    "**Null:** Control and treatment samples are part of the same binomial distribution. Treatment prob - Control prob <= 0.\n",
    "\n",
    "**Alt:** Treatment sample is part of a binomial distribution with mean higher than that of control. Treatment prob - Control prob > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Derive Stat Power from Probs and Sample Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a: Simulation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get variances of binomial variables based on treatment and control probs\n",
    "# REMEMBER: variance of binomial variable is k(p)(1-p), where k is count of trials\n",
    "# In a marketing experiment trial count per sample member is 1 (only one chance to click or not per member)\n",
    "# In light of hypotheses, we need to consider both control and treatment as parts of the same distribution\n",
    "# So, we combine their binomial distributions Control + Treatment\n",
    "# Variances are likewise added Var(Treatment) + Var(Control)\n",
    "variance_control = 1 * prob_control * (1 - prob_control)\n",
    "variance_treatment = 1 * prob_treatment * (1 - prob_treatment)\n",
    "variance_null = variance_control + variance_treatment\n",
    "\n",
    "# Get standard error of null distribution\n",
    "# Using control for both elements of calculation, because null hypothesis is that Treatment + Control is same as Control + Control\n",
    "# Both treatment and control are parts of equivalent binomial distributions\n",
    "sterror_null = np.sqrt((variance_control / size_control) + (variance_control / size_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Difference: 0.022068027137403433\n"
     ]
    }
   ],
   "source": [
    "# Get the null binomial distribution\n",
    "null_dist = stats.norm(loc = 0, scale = sterror_null)\n",
    "p_crit = null_dist.ppf(1 - alpha)\n",
    "print(\"Critical Difference: {}\".format(p_crit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard error of alt distribution (assumes treatment - prob is mean of its own separate binomial variable)\n",
    "sterror_alt = np.sqrt((variance_control / size_control) + (variance_treatment / size_treatment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat Power: 0.44122379261151545\n"
     ]
    }
   ],
   "source": [
    "# Get the alt binomial distribution\n",
    "alt_dist = stats.norm(loc = prob_treatment - prob_control, scale = sterror_alt)\n",
    "beta = alt_dist.cdf(p_crit) # cumulative distribution function. Proportion of values in distribution below given value.\n",
    "stat_power = 1 - beta # inverts beta to get proportion of values in alt_dist above p_crit\n",
    "\n",
    "print(\"Stat Power: {}\".format(stat_power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Derive Sample Size from Stat Power and Probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Analytic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify inputs\n",
    "alpha = 0.05\n",
    "power = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sig Level: 0.05\n",
      "Target Power: 0.8\n",
      "Target Group Size: 1,613\n",
      "Target Experiment Size: 3,226\n"
     ]
    }
   ],
   "source": [
    "z_null = stats.norm.ppf(1-alpha)\n",
    "z_alt = stats.norm.ppf(power)\n",
    "\n",
    "stdev_null = np.sqrt((prob_control * (1 - prob_control)) + (prob_control * (1 - prob_control)))\n",
    "stdev_alt = np.sqrt((prob_control * (1 - prob_control)) + (prob_treatment * (1 - prob_treatment)))\n",
    "\n",
    "z_diff = z_null - z_alt\n",
    "p_diff = treatment_prob - control_prob\n",
    "\n",
    "n = int(np.ceil((z_diff / p_diff) ** 2))\n",
    "\n",
    "print(\"Target Sig Level: {}\".format(alpha))\n",
    "print(\"Target Power: {}\".format(power))\n",
    "print(\"Target Group Size: {:,}\".format(n))\n",
    "print(\"Target Experiment Size: {:,}\".format(2*n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
